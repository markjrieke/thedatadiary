---
title: "Momentum in Presidential Elections"
author: "Mark Rieke"
date: "9/26/2021"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# libraries 
library(tidyverse)
library(tidymodels)
library(vip)

# themes
source("https://raw.githubusercontent.com/markjrieke/thedatadiary/main/dd_theme_elements/dd_theme_elements.R")

# path setup
# path <- "2021.09.26-president_momentum/"
```

When discussing elections, horserace coverage tends to frame a state's expected outcome based on how it's most recent voting patterns compare to previous elections. This is, in part, why Florida is framed as "trending towards Republicans" and Texas is framed as "trending towards Democrats," despite the fact that in the 2020 presidential election, Trump won Texas by a wider margin than Florida!

```{r fl/tx plot, message=FALSE}
read_csv("data/mit_labs_1976_2020_president.csv") %>%
  filter(state %in% c("FLORIDA", "TEXAS"),
         party_simplified %in% c("DEMOCRAT", "REPUBLICAN")) %>%
  select(year, state, candidatevotes, party_simplified) %>%
  group_by(year, state) %>%
  mutate(dem_pct = candidatevotes/sum(candidatevotes)) %>%
  filter(party_simplified == "DEMOCRAT") %>%
  ungroup() %>%
  select(year, state, dem_pct) %>%
  ggplot(aes(x = year,
             y = dem_pct,
             color = state)) +
  geom_line(size = 1,
            alpha = 0.75) +
  theme_minimal(base_family = "Roboto Slab") +
  theme(plot.subtitle = element_markdown(family = "Roboto Slab"),
        plot.background = element_rect(fill = "white",
                                       color = "white")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  geom_hline(yintercept = 0.5,
             color = dd_gray,
             size = 1,
             linetype = "dashed",
             alpha = 0.2) +
  scale_color_manual(values = c(dd_blue, dd_green)) +
  theme(legend.position = "none",
        plot.title.position = "plot") +
  labs(title = "Momentum in Presidential Elections",
       subtitle = "Democratic Two-party Presidential Voteshare in <span style=color:'#5565D7'>**Florida**</span> and <span style=color:'#65D755'>**Texas**</span> since 1976",
       x = NULL,
       y = NULL)

ggsave("pics/tx_fl_2pv.png",
       width = 9,
       height = 6,
       units = "in",
       dpi = 500)
```

That certainly *feels* like a compelling narrative, but I'm skeptical as to whether these trends can be explained by momentum My hunch is that other factors, like changing demographics, national sentiment, & presidential approval, can explain these trends & that momentum itself offers little explanatory power. To test this hunch, I'll build and evaluate a few machine learning models with R's suite of machine learning packages, [`tidymodels`](https://www.tidymodels.org/). 

## Exploring the Data

I've put together a dataset for examining presidential elections at the state level since 1960 (for information on the dataset, you can reference the [notes](https://github.com/markjrieke/thedatadiary/tree/main/2021.09.26-president_momentum/notes); to see how it was put together, reference [this script](https://github.com/markjrieke/thedatadiary/blob/main/2021.09.26-president_momentum/scripts/data_wrangle.R)). In addition to the usual host of economic & demographic predictor variables, I've added two "momentum" variables - one showing the change in the democratic voteshare since the previous presidential election and another showing the change from two elections ago. 

```{r model_data display, message=FALSE}
read_csv("data/model_data/model_data.csv") %>%
  filter(state == "Texas") %>%
  knitr::kable(caption = "Model Data (Filtered to Texas")
```

Let's look into how some of these variables relate to each other!

```{r EDA-1, message=FALSE}
elections <- read_csv("data/model_data/model_data.csv")

elections %>%
  select(d_pct, ends_with("cycle")) %>%
  pivot_longer(cols = ends_with("cycle"),
               names_to = "cycle",
               values_to = "momentum") %>%
  ggplot(aes(x = momentum,
             y = d_pct)) +
  geom_point(alpha = 0.2,
             color = "midnightblue") +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~cycle) +
  theme(legend.position = "none")
  
```

Right off the bat, it looks like there *is* a relationship between the newly created momentum variables and the democratic voteshare, though *how* important each variable is to the overall model remains to be determined. As it stands now, however, we can't simply rule out momentum as a factor prior to analysis, since there clearly is some correlation with vote outcome. 

```{r EDA-2}
elections %>%
  select(d_pct, aapi, black, hispanic, white, native_american) %>%
  pivot_longer(cols = -d_pct,
               names_to = "demo",
               values_to = "demo_pct") %>%
  ggplot(aes(x = demo_pct,
             y = d_pct)) +
  geom_point(alpha = 0.2,
             color = "midnightblue") +
  facet_wrap(~demo, scales = "free_x") +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none")

```

Looking at the how demographics influence outcomes, there may be an opportunity for some interesting [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) later down the line. That being said, there is likely a good amount of redundant information in these charts, since the total percentage always has to add up to 100%.

```{r EDA-3}
elections %>%
  select(d_pct, inflation, gdp_growth) %>%
  pivot_longer(cols = -d_pct,
               names_to = "metric",
               values_to = "value") %>%
  ggplot(aes(x = value,
             y = d_pct)) +
  geom_point(alpha = 0.2,
             color = "midnightblue") +
  facet_wrap(~metric, scales = "free_x") +
  theme(legend.position = "none")
```

It looks like there's a bit of non-linearity between the economic factors & the democratic voteshare. Since these two measurements are showing the change in GDP & inflation over the previous 4 years, it may make sense to check for an [interaction](https://en.wikipedia.org/wiki/Interaction_(statistics)) with the variable `previous_president_party`.

```{r EDA-4}
elections %>%
  select(d_pct, inflation, gdp_growth, previous_presidential_party) %>%
  pivot_longer(cols = c(inflation, gdp_growth),
               names_to = "metric",
               values_to = "value") %>%
  ggplot(aes(x = value,
             y = d_pct,
             color = previous_presidential_party)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~metric, scales = "free_x") +
  scale_color_manual(values = c(dd_blue, dd_red)) +
  theme(legend.position = "none")
```

Hmm - I'm not noticing any distinctly differentiable interactions between the economic variables, but may be worthwhile to check later. 

```{r}
elections %>%
  select(d_pct, democratic_incumbent_run, republican_incumbent_running) %>%
  pivot_longer(cols = -d_pct,
               names_to = "party",
               values_to = "incumbent") %>%
  ggplot(aes(x = incumbent,
             y = d_pct,
             color = party)) +
  geom_boxplot() +
  geom_point(position = position_jitterdodge(),
             alpha = 0.2) +
  facet_wrap(~party) +
  theme(legend.position = "none") +
  scale_color_manual(values = c(dd_blue, dd_red))
```

I wonder what this would look like with a third category for "no incumbent"

```{r}
elections %>%
  select(d_pct, democratic_incumbent_run, republican_incumbent_running) %>%
  mutate(no_incumbent = if_else(democratic_incumbent_run == "No" & republican_incumbent_running == "No", "Yes", "No")) %>%
  pivot_longer(cols = -d_pct,
               names_to = "party",
               values_to = "incumbent") %>%
  ggplot(aes(x = incumbent,
             y = d_pct,
             color = party)) +
  geom_boxplot() +
  geom_point(position = position_jitterdodge(),
             alpha = 0.2) +
  facet_wrap(~party) +
  theme(legend.position = "none") +
  scale_color_manual(values = c(dd_blue, dd_purple, dd_red))
```

That may be a worthwhile feature to add in later on - it looks like there may be some meaningful information within "no incumbent."

```{r}
elections %>%
  select(d_pct, starts_with("previous")) %>%
  ggplot(aes(x = previous_presidential_party,
             y = d_pct)) +
  geom_boxplot() +
  geom_point(alpha = 0.2,
             color = "midnightblue",
             position = position_jitter())
```

Mildly surprised by this - I would have assumed there was a penalty for the previous president's party. I guess, however, this isn't taking into consideration incumbents running for reelection.

```{r}
elections %>%
  select(d_pct, starts_with("previous")) %>%
  select(-previous_presidential_party) %>%
  pivot_longer(cols = starts_with("previous"),
               names_to = "rating_type",
               values_to = "rating") %>%
  ggplot(aes(x = rating,
             y = d_pct)) +
  geom_point(color = "midnightblue",
             alpha = 0.2) +
  facet_wrap(~rating_type,
             scales = "free_x")
```

Hmmm - let's check for interaction with the previous presidential party

```{r}
elections %>%
  select(d_pct, starts_with("previous")) %>%
  pivot_longer(cols = starts_with("previous_president_"),
               names_to = "rating_type",
               values_to = "rating") %>%
  ggplot(aes(x = rating,
             y = d_pct,
             color = previous_presidential_party)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~rating_type,
             scales = "free_x") +
  theme(legend.position = "none") +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(values = c(dd_blue, dd_red))
```

Definitely will want to fit an interactive term for each of the approval ratings!

```{r}
elections %>%
  select(d_pct, d_pct_nat) %>%
  ggplot(aes(x = d_pct_nat,
             y = d_pct)) +
  geom_point(alpha = 0.2,
             color = "midnightblue") +
  geom_smooth(method = "lm", se = FALSE)
```

Obviously, an expected relationship.  Let's get on to the fun part: modeling. Here's the basic game plan:

Using all the variables listed above, I'll train the following models to predict the Democratic voteshare (`d_pct`) in each state and check the importance of each variable along the way:

* Basic Logistic Regression
* Regularized Logistic Regression (without any feature engineering)
* Tuned Logistic Regression (with feature engineering)

## Modeling
### Basic Logistic Regression

Firstly, we'll need to split the data into testing & training data

```{r}
# set seed for reproducability
set.seed(123)

# split into testing & training data
elections_split <- initial_split(elections, prop = 0.8)
elections_train <- training(elections_split)
elections_test <- testing(elections_split)
```

While we won't be tuning this first model, it'll be good to evaluate on a holdout set of data. We don't want to use the testing data, so I'll pull some cross validation resamples from the training set.

```{r}
set.seed(10101)
elections_boot <- bootstraps(elections_train)
```

Now I can define a specification for the logistic regression. Setting the `glmnet::family` parameter to `binomial(link = "logit")` means that we can use a generalized linear model with a logistic link (aka, a logistic regression). The `parsnip::logistic_reg()` function only allows for the a logistic regression to be used for classification, which is not what we want in this case (we want to predict the actual voteshare outcome, not whether or not the democratic candidate won). 

```{r}
basic_spec <-
  linear_reg(mode = "regression",
             penalty = 0) %>%
  set_engine("glmnet",
             family = binomial(link = "logit"))
```

Now we can add a basic preprocessing recipe:

```{r}
basic_rec <-
  recipe(d_pct ~ ., data = elections_train) %>%
  update_role(year, state, new_role = "id") %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
```

Finally, Let's fit & explore the results!

```{r}
set.seed(1540)

basic_rs <-
  workflow() %>%
  add_recipe(basic_rec) %>%
  add_model(basic_spec) %>%
  fit_resamples(resamples = elections_boot,
                control = control_resamples(save_pred = TRUE))
```

Some problems with convergence on a few resamples - but let's check metrics:

```{r}
basic_rs %>%
  collect_metrics()

```


```{r}
p_1 <- 
  basic_rs %>%
  collect_predictions() %>%
  ggplot(aes(x = .pred,
             y = d_pct,
             color = id)) +
  geom_point(alpha = 0.2) +
  geom_abline(linetype = "dashed",
              size = 0.8,
              alpha = 0.5,
              color = dd_gray) +
  theme_minimal() +
  theme(legend.position = "none") +
  geom_smooth(method = "lm") +
  facet_wrap(~id) 

p_1
```

So pretty systematically, the basic regression is overestimating `d_pct` at lower percentage predictions and underestimating `d_pct` at higher percentage predictions (it's not expected that this performs perfect right away!). Let's see which variables are the most important in this basic model:

```{r}
basic_vip <-
  finalize_workflow(
    workflow() %>% add_recipe(basic_rec) %>% add_model(basic_spec),
    basic_rs %>% select_best("rmse", maximize = FALSE)
  )

basic_vip %>%
  fit(elections_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = 0) %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none")

basic_vip %>%
  fit(elections_train) %>%
  pull_workflow_fit() %>% 
  vi(lambda = 0) %>%
  mutate(Importance = if_else(Sign == "NEG", -1 * Importance, Importance),
         Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Variable,
             y = Importance,
             fill = Sign)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none")

```

We're way overfitting here, but look at that! 1/2 cycle momentum actually *is* contributing! It'll be interesting to see how this changes as we add in regularization & feature engineering. 

### Regularized Logistic Regression

We'll be using the same recipe for the regularized regression, but we'll tune the `penalty` (how much the model penalizes/regularizes the addition of new predictors) and `mixture` (the proportion of lasso/ridge regularization the model will use) **hyperparameters**. Let's setup the recipe and spec.

```{r}
tune_rec <- basic_rec

tune_spec <- 
  linear_reg(mode = "regression",
             penalty = tune(),
             mixture = tune()) %>%
  set_engine("glmnet",
             family = binomial(link = "logit"))
```

Now, we'll tune a regular grid across our bootstrap resamples:

```{r}
# use multiple cores for tuning resamples
doParallel::registerDoParallel()

set.seed(999)

tune_rs <-
  workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(tune_rec) %>%
  tune_grid(resamples = elections_boot,
            grid = grid_regular(penalty(), mixture()),
            control = control_resamples(save_pred = TRUE))
```




